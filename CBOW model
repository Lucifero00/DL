import re
import random
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Lambda, Dense
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import seaborn assns

np.random.seed(42)
tf.random.set_seed(42)
random.seed(42)

data = """The Skip-Gram model is another approach used in natural language processing to create word embeddings. 
Unlike the Continuous Bag of Words, Skip-Gram predicts surrounding words given a target word, 
allowing it to capture more contextual relationships. These embeddings can then be applied to various NLP tasks such 
as sentiment analysis, machine translation, and information retrieval."""

sentences = [s.strip() for s in data.split('.') if s.strip()]

clean_sent = []
for sentence in sentences:
    s = re.sub('[^A-Za-z0-9 ]+', ' ', sentence)
    s = re.sub(r'(?:^| )\w(?:$| )', ' ', s).strip()
    s = s.lower()
    if s:
        clean_sent.append(s)

tokenizer = Tokenizer(oov_token=None)
tokenizer.fit_on_texts(clean_sent)
sequences = tokenizer.texts_to_sequences(clean_sent)
word_to_index = tokenizer.word_index
index_to_word = {i: w for w, i in word_to_index.items()}

vocab_size = len(word_to_index) + 1
print("Vocab size:", vocab_size)
print("Word to index (sample):", dict(list(word_to_index.items())[:10]))

context_size = 2
contexts = []
targets = []

for seq in sequences:
    for i in range(context_size, len(seq) - context_size):
        target = seq[i]
        context = seq[i - context_size:i] + seq[i+1:i+1+context_size]
        if len(context) == 2*context_size:
            contexts.append(context)
            targets.append(target)

contexts = np.array(contexts)
targets = np.array(targets)
print("Number of training pairs:", len(targets))
print("Example pair (context indices -> target index):", contexts[0], "->", targets[0])

input_length = contexts.shape[1]

emb_size = 10

model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=emb_size, input_length=input_length, name="embedding"),
    Lambda(lambda x: tf.reduce_mean(x, axis=1), name="mean_pool"),
    Dense(128, activation='relu'),
    Dense(vocab_size, activation='softmax')
])

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

history = model.fit(contexts, targets, epochs=200, batch_size=8, verbose=0)

plt.figure(figsize=(8,3))
plt.subplot(1,2,1)
plt.plot(history.history['loss']); plt.title('Loss'); plt.xlabel('epoch')
plt.subplot(1,2,2)
plt.plot(history.history['accuracy']); plt.title('Accuracy'); plt.xlabel('epoch')
plt.tight_layout(); plt.show()

embeddings = model.get_layer("embedding").get_weights()[0]

emb_vectors = embeddings[1:]
words = [index_to_word[i] for i in range(1, vocab_size)]

pca = PCA(n_components=2)
reduced = pca.fit_transform(emb_vectors)

plt.figure(figsize=(8,6))
sns.scatterplot(x=reduced[:,0], y=reduced[:,1])
for i, w in enumerate(words):
    plt.text(reduced[i,0]+0.01, reduced[i,1]+0.01, w, fontsize=9)
plt.title("Word embeddings (PCA 2D)")
plt.show()

def predict_from_context(context_words):
    idxs = [word_to_index.get(w, 0) for w in context_words]
    
    if len(idxs) < input_length:
        idxs = idxs + [0] * (input_length - len(idxs))
    else:
        idxs = idxs[:input_length]
    x = np.array([idxs])
    preds = model.predict(x, verbose=0)[0]
    top3 = preds.argsort()[-3:][::-1]
    return [(index_to_word.get(i, "<PAD>"), preds[i]) for i in top3]

example_context = ["skip", "gram", "predicts", "surrounding"]
print("Context words:", example_context)
print("Top predictions:", predict_from_context(example_context))

example_context2 = ["natural","language","processing","technique"]
print("Top predictions:", predict_from_context(example_context2))
